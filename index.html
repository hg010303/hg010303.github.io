<!doctype html>
<html>
<head>
<!--
	 Global site tag (gtag.js) - Google Analytics 
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-141762792-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-141762792-1');
</script>
-->

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Honggyu An's Homepage</title>
<link href="AboutPageAssets/styles/aboutPageStyle.css" rel="stylesheet" type="text/css">
<style type="text/css">
</style>

<!--The following script tag downloads a font from the Adobe Edge Web Fonts server for use within the web page. We recommend that you do not modify it.-->
<script>var __adobewebfontsappname__="dreamweaver"</script><script src="http://use.edgefonts.net/montserrat:n4:default;source-sans-pro:n2:default.js" type="text/javascript"></script>
</head>

<body alink = "#282727" vlink = "#9b9b9b" link = "#9b9b9b">
<!-- Header content -->
<header>
  <div class="profilePhoto"> 
    <!-- Profile photo --> 
    <img src="AboutPageAssets/images/profile_1.png" alt="sample" width="259"> </div>
<!--	<img src="AboutPageAssets/images/fonitAigolkatan.jpg" alt="sample" width="259"> </div>-->
  <!-- Identity details -->
  <section class="profileHeader">
    <h1>Honggyu An</h1>
<!--    <h3>Ms. Student</h3>-->
    <hr>
    <p>I am 1st-year integrated MS/PH.D student Kim at the KAIST under the supervision of Prof. <a href="https://cvlab.korea.ac.kr/" target="_blank">
      Seungryong Kim</a>. My research is in computer vision and deep learning, I am particularly interested in Depth estimation and NeRF.
	<br>
    </p>
  </section>
  <!-- Links to Social network accounts -->
  <aside class="socialNetworkNavBar">
	   <div class="socialNetworkNav"> 
      <!-- Add a Anchor tag with nested img tag here --> 
		   <a href="honggyu@kaist.ac.kr">
      <img src="AboutPageAssets/images/mail.png"  alt="sample" width="30" ></a> </div>
      <div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://github.com/hg010303" target="_blank">
      <img src="AboutPageAssets/images/github.png" alt="sample" width="30"></a>  </div>

  </aside>
</header>
<!-- content -->
<section class="mainContent"> 
  <!-- Contact details -->
  
  <!-- Previous experience details -->
  
  <section class="section2">
  <h2 class="sectionTitle">Publications</h2>
    <hr class="sectionTitleRule">
    <hr class="sectionTitleRule2">
	  	  <!-- Lumiere  -->
		<div class="sectionContent">
			<img align="center" src="AboutPageAssets/teasers/zeroco.png" style="height: 100%; width: 100%; object-fit: contain"  alt="">
			</div>
		<section class="section2Content">
				<h2 class="sectionContentTitle"> Cross-View Completion Models are Zero-shot Correspondence Estimators </h2>
				<h3 class="sectionContentSubTitle"><b>Honggyu An*</b>, Jinhyeon Kim*, Seonghoon Park, Jaewoo Jung, Jisang Han, Sunghwan Hong, Seungryong Kim
				</h3>
				<h3 class="sectionContentSubTitle"><em>CVPR, <strong>Highlight</strong> (3.0% Acceptance Rate), 2025<b style="color:darksalmon"></b>
				</em></h3>
		</section>
		<aside class="externalResourcesNav">

			<div class="dropdown"><span></span><a href="https://cvlab-kaist.github.io/ZeroCo/" target="_blank">Project Page</a></div>
			<div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
				<p style="text-align:left;">In this work, we explore new perspectives on cross-view completion learning by drawing an analogy to self-supervised correspondence learning. Through our analysis, we demonstrate that the cross-attention map within cross-view completion models captures correspondence information more effectively than other correlations derived from encoder or decoder features. We verify the effectiveness of the cross-attention map by evaluating on both zero-shot matching and learning-based geometric matching and multi-frame depth estimation.</p></div>
			</div>
			<div class="dropdown"><span></span><a href="https://arxiv.org/abs/2412.09072" target="_blank">Paper</a></div>
			<div class="dropdown"><span></span><a href="https://github.com/cvlab-kaist/ZeroCo" target="_blank">Code</a></div>
		</aside>

		<br><br>
		<br><br>


		<div class="sectionContent">
			<img align="center" src="AboutPageAssets/teasers/locotrack.png" style="height: 100%; width: 100%; object-fit: contain"  alt="">
			</div>
		<section class="section2Content">
				<h2 class="sectionContentTitle"> Local All-Pair Correspondence for Point Tracking </h2>
				<h3 class="sectionContentSubTitle">Seokju Cho, Jiahui Huang, Jisu Nam, <b>Honggyu An</b>, Seungryong Kim<sup>&dagger;</sup>, Joon-Young Lee<sup>&dagger;</sup>
				</h3>
				<h3 class="sectionContentSubTitle"><em>ECCV, 2024<b style="color:darksalmon"></b>
				</em></h3>
		</section>
		<aside class="externalResourcesNav">

			<div class="dropdown"><span></span><a href="https://cvlab-kaist.github.io/locotrack/" target="_blank">Project Page</a></div>
			<div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
				<p style="text-align:left;">We introduce LocoTrack, a highly accurate and efficient model designed for the task of tracking any point (TAP) across video sequences. Previous approaches in this task often rely on local 2D correlation maps to establish correspondences from a point in the query image to a local region in the target image, which often struggle with homogeneous regions or repetitive features, leading to matching ambiguities. LocoTrack overcomes this challenge with a novel approach that utilizes all-pair correspondences across regions, i.e., local 4D correlation, to establish precise correspondences, with bidirectional correspondence and matching smoothness significantly enhancing robustness against ambiguities. We also incorporate a lightweight correlation encoder to enhance computational efficiency, and a compact Transformer architecture to integrate long-term temporal information. LocoTrack achieves unmatched accuracy on all TAP-Vid benchmarks and operates at a speed almost 6 times faster than the current state-of-the-art.</p></div>
			</div>
			<div class="dropdown"><span></span><a href="https://arxiv.org/abs/2407.15420" target="_blank">Paper</a></div>
			<div class="dropdown"><span></span><a href="https://github.com/cvlab-kaist/locotrack" target="_blank">Code</a></div>
		</aside>

		<br><br>
		<br><br>

		<div class="sectionContent">
			<img align="center" src="AboutPageAssets/teasers/rain_gs.png" style="height: 100%; width: 100%; object-fit: contain"  alt="">
	  	</div>
		<section class="section2Content">
      		<h2 class="sectionContentTitle"> RAIN-GS: Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting</h2>
      		<h3 class="sectionContentSubTitle">Jaewoo Jung*, Jisang Han*, <b>Honggyu An*</b>, Jiwon Kang*, Seonghoon Park*, Seungryong Kim
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>Arxiv, 2024<b style="color:darksalmon"></b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/RAIN-GS/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">3D Gaussian splatting (3DGS) has recently demonstrated impressive capabilities in real-time novel view synthesis and 3D reconstruction. However, 3DGS heavily depends on the accurate initialization derived from Structure-from-Motion (SfM) methods. When trained with randomly initialized point clouds, 3DGS often fails to maintain its ability to produce high-quality images, undergoing large performance drops of 4-5 dB in PSNR in general. Through extensive analysis of SfM initialization in the frequency domain and analysis of a 1D regression task with multiple 1D Gaussians, we propose a novel optimization strategy dubbed RAIN-GS (Relaxing Accurate INitialization Constraint for 3D Gaussian Splatting) that successfully trains 3D Gaussians from randomly initialized point clouds. We show the effectiveness of our strategy through quantitative and qualitative comparisons on standard datasets, largely improving the performance in all settings.</p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="http://arxiv.org/abs/2403.09413" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/RAIN-GS" target="_blank">Code</a></div>
		</aside>

	  <br><br>
	  <br><br>


	  	<div class="sectionContent">
	 		<img align="center" src="AboutPageAssets/teasers/maskingdepth.png" style="height: 100%; width: 100%; object-fit: contain"  alt="">

	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle">MaskingDepth: Masked Consistency Regularization for Semi-supervised Monocular Depth Estimation</h2>
      		<h3 class="sectionContentSubTitle">Jongbeom Baek*,Gyeongnyeon Kim*, Seonghoon Park*,<b>Honggyu An</b>,Matteo Poggi, Seungryong Kim
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024<b style="color:darksalmon"></b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/MaskingDepth/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">
				We propose MaskingDepth, a novel semi-supervised learning framework for monocular depth estimation to mitigate the reliance on large ground-truth depth quantities. MaskingDepth is designed to enforce consistency between the strongly-augmented unlabeled data and the pseudo-labels derived from weakly-augmented unlabeled data, which enables learning depth without supervision. In this framework, a novel data augmentation is proposed to take the advantage of a na ̈ıve masking strategy as an augmentation, while avoiding its scale ambiguity problem between depths from weakly- and strongly-augmented branches and risk of missing small-scale instances. To only retain high-confident depth predictions from the weakly augmented branch as pseudo-labels, we also present an uncertainty estimation technique, which is used to define robust consistency regularization. Experiments on KITTI and NYU-Depth-v2 datasets demonstrate the effectiveness of each component, its robustness to the use of fewer depth- annotated images, and superior performance compared to other state-of-the-art semi-supervised methods for monocular depth estimation. Furthermore, we show our method can be easily extended to domain adaptation task.
			  </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2212.10806" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/MaskingDepth" target="_blank">Code</a></div>
		</aside>

	  <br><br>
	  <br><br>
	  <!-- SMM  -->
	  	<div class="sectionContent">
	 		<img align="center" src="AboutPageAssets/teasers/DaRF.png" style="height: 100%; width: 100%; object-fit: contain"  alt="">
	  	</div>
		<section class="section2Content">
      		<h2 class="sectionContentTitle"> DäRF: Boosting Few-shot Neural Radiance Field with Joint Monocular Depth Adaptation</h2>
      		<h3 class="sectionContentSubTitle">Jiuhn Song*, Seonghoon Park*, <b>Honggyu An*</b>, Seokju Cho, Min-Seop Kwak, Sungjin Cho, Seungryong Kim
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>Neural Information Processing Systems (Neurips), 2023<b style="color:darksalmon"></b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/DaRF/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">Neural radiance fields (NeRF) shows powerful performance in novel view synthesis and 3D geometry reconstruction, but it suffers from critical performance degradation when the number of known viewpoints is drastically reduced. Existing works attempt to overcome this problem by employing external priors, but their success is limited to certain types of scenes or datasets. Employing monocular depth estimation (MDE) networks, pretrained on large-scale RGB-D datasets, with powerful generalization capability would be a key to solving this problem: however, using MDE in conjunction with NeRF comes with a new set of challenges due to various ambiguity problems exhibited by monocular depths. In this light, we propose a novel framework, dubbed DäRF, that achieves robust NeRF reconstruction with a handful of real-world images by combining the strengths of NeRF and monocular depth estimation through online complementary training. Our framework imposes the MDE network's powerful geometry prior to NeRF representation at both seen and unseen viewpoints to enhance its robustness and coherence. In addition, we overcome the ambiguity problems of monocular depths through patch-wise scale-shift fitting and geometry distillation, which adapts the MDE network to produce depths aligned accurately with NeRF geometry. Experiments show our framework achieves state-of-the-art results both quantitatively and qualitatively, demonstrating consistent and reliable performance in both indoor and outdoor real-world datasets.</p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2305.19201" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/DaRF" target="_blank">Code</a></div>
		</aside>

	  <br><br>
	  <br><br>

	  <!-- SMM  -->

	
<!-- Replicate the above Div block to add more title and company details --> 
	</section>
<!--	<ul style="color: slategrey">-->
<!--  	<li>Splicing ViT Features: TAU Graphics Seminar 2022</li>-->
<!--  	<li>Text2LIVE: HUJI Vision Seminar 2022</li>-->
<!--	<li>Text2LIVE: Israel Computer Vision day 2022</li>-->
<!--	<li>Unveiling new priors for re-rendering images and videos: TAU Deep Learning Seminar 2023</li>-->
<!--	<li>Text2LIVE: Viz.ai 2023</li>-->
<!--</ul>-->
</section>
<footer>
	<br><br>
  <p class="footerDisclaimer">Template of <a href="https://lioryariv.github.io/">Lior Yariv</a><span></span></p>
  <p class="footerNote"></p>
</footer>
</body>
</html>
