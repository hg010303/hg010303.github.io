<!doctype html>
<html>
<head>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nunito:wght@200..1000&family=Open+Sans:wght@300..800&family=Roboto:wght@100..900&family=Winky+Sans:wght@300..900&display=swap" rel="stylesheet">

<style>
  /* ====== Font Styling ====== */
  body, h1, h2, h3, h4, h5, h6, p, a, span, div, li, button, input, textarea, section, footerNote, footerDisclaimer {
    font-family: 'Nunito', sans-serif !important;
  }

  /* ====== Font Sizes ====== */
  .profileHeader { font-size: 16px !important; } /* Introduction */
  .sectionTitle { font-size: 20px !important; }  /* Section Title */
  .section1 { font-size: 17px; } /* Work Experience */
  .sectionContentTitle { font-size: 18px !important; } /* Paper Title */
  .sectionContentSubTitle { font-size: 17px !important; } /* Author Names */
  .dropdown { font-size: 17px !important; }  /* Gray Box */

  /* ====== Remove Space Between Paper Name and Author Names ====== */
  .sectionContentTitle { margin-bottom: 0 !important; }
  .sectionContentSubTitle { margin-top: 0 !important; }

  /* ====== Image Centering and Scaling ====== */

  .sectionContent img { 
    width: 100% !important; 
    height: auto !important;
	justify-content: left !important;
    align-items: left !important;
  }
  
  /* ====== Profile and Text ====== */
  
 .profilePhoto{
		margin-bottom: 120px !important; /* Adds space between the image and text */
	}

	.adobe-logo {
		width: 18px; /* Adjust the logo size here (smaller value) */
		vertical-align: middle; /* Align logo with the text */
		margin-right: 2px; /* Space between the logo and text */
	}

	.naver-logo {
		width: 18px; /* Adjust the logo size here (smaller value) */
		vertical-align: middle; /* Align logo with the text */
		margin-right: 2px; /* Space between the logo and text */
	}

</style>

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Honggyu An's Homepage</title>
<link href="AboutPageAssets/styles/aboutPageStyle.css" rel="stylesheet" type="text/css">
<style type="text/css">
</style>

<!--The following script tag downloads a font from the Adobe Edge Web Fonts server for use within the web page. We recommend that you do not modify it.-->
<script>var __adobewebfontsappname__="dreamweaver"</script><script src="http://use.edgefonts.net/montserrat:n4:default;source-sans-pro:n2:default.js" type="text/javascript"></script>
</head>

<body alink = "#282727" vlink = "#9b9b9b" link = "#9b9b9b">
<!-- Header content -->
<header>
  <div class="profilePhoto"> 
    <!-- Profile photo --> 
	<br>
	<br>
    <img src="AboutPageAssets/images/profile_1.png" alt="sample" width="263"> </div>

  <!-- Identity details -->
  <section class="profileHeader">
    <h1>Honggyu An</h1>
    <hr>
    <p>I am 2nd-year integrated MS/PH.D student at the KAIST under the supervision of Prof. <a href="https://cvlab.korea.ac.kr/" target="_blank">
      Seungryong Kim</a>. I am closely collaborated with Takuya Narihira and Kaz Fukuda at Sony AI. 
	 <br> 
	  My research interesting is scene reconstruction and understanding in static and dynamic environments, particulary 3D/4D reconstruction, 3D feature lifting and visual correspondences.
	<br>
    </p>
  </section>
  <!-- Links to Social network accounts -->
  <aside class="socialNetworkNavBar">
	   <div class="socialNetworkNav"> 
      <!-- Add a Anchor tag with nested img tag here --> 
	<a href="AboutPageAssets/HonggyuAn_CV_20251216.pdf" target="_blank">
      <img src="AboutPageAssets/images/cv.png" alt="sample" width="30"></a>  </div>
	<div class="socialNetworkNav">
		<a href="honggyu@kaist.ac.kr">
      <img src="AboutPageAssets/images/mail.png"  alt="sample" width="30" ></a> </div>
      <div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://github.com/hg010303" target="_blank">
      <img src="AboutPageAssets/images/github.png" alt="sample" width="30"></a>  </div>
		<div class="socialNetworkNav"> 
		<a href="https://scholar.google.co.kr/citations?user=W69-kzMAAAAJ&hl=en" target="_blank">
      <!-- Add a Anchor tag with nested img tag here --> 
      <img src="AboutPageAssets/images/scholar.jpg"  alt="sample" width="30"></a>  </div>

  </aside>
</header>
<!-- content -->
<section class="mainContent"> 
  <!-- Contact details -->
  
  <!-- Previous experience details -->
  
  <section class="section2">
  <h2 class="sectionTitle">Publications</h2>
    <hr class="sectionTitleRule">
    <hr class="sectionTitleRule2">
		<!-- C3G -->
		<div class="sectionContent">
			<img src="AboutPageAssets/teasers/c3g.png" alt="DiffTrack Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle">C3G: Learning Compact 3D Representations with 2K Gaussian</h2>

      		<h3 class="sectionContentSubTitle"><b><a>Honggyu An<sup>*</sup></a></b>, <a>Jaewoo Jung<sup>*</sup></a>, <a>Mungyeom Kim</a>, <a>Sunghwan Hong</a>, <a>Chaehyun Kim</a>, <a>Kazumi Fukuda</a>, <a>Minkyeong Jeon</a>, <a>Jisang Han</a>, <a>Takuya Narihira</a>, <a>Hyuna Ko</a>, <a>Junsu Kim</a>, <a>Yuki Mitsfuji<sup>&dagger;</sup></a>, <a>Seungryong Kim<sup>&dagger;</sup></a></h3>
		  	<h3 class="sectionContentSubTitle"><em> arXiv, 2025 </em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://cvlab-kaist.github.io/C3G/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>TL;DR</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">We propose a feed-forward framework for learning compact 3D representations from unposed images. Our approach estimates only 2K Gaussians that allocated in meaningful regions to enable generalizable scene reconstruction and understanding. </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2512.04021" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/cvlab-kaist/C3G" target="_blank"><b>Code</b></a></div>
		</aside>

	  <br><br>

		<!-- DitTracker -->
		<div class="sectionContent">
			<img src="AboutPageAssets/teasers/DiTracker.png" alt="DiffTrack Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle">DiTracker: Repurposing Video Diffusion Transformers for Robust Point Tracking</h2>

      		<h3 class="sectionContentSubTitle"><a>Soowon Son</a>, <b><a>Honggyu An</a></b>, <a>Chaehyun Kim</a>, <a>Hyunah Ko</a>, <a>Jisu Nam</a>, <a>Dahyun Chung</a>, <a>Siyoon Jin</a>, <a>Jung Yi</a>, <a>Jaewon Min</a>, <a>Junhwa Hur<sup>&dagger;</sup></a>, <a>Seungryong Kim<sup>&dagger;</sup></a></h3>
		  	<h3 class="sectionContentSubTitle"><em> arXiv, 2025 </em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://cvlab-kaist.github.io/DiTracker/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>TL;DR</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">DiTracker repurposes video DiTs for point tracking with softmax-based matching, LoRA adaptation, and cost fusion, achieving stronger robustness and faster convergence </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2512.20606" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/cvlab-kaist/DiTracker" target="_blank"><b>Code</b></a></div>
		</aside>

	  <br><br>

		<!-- Robust VGGT -->
		<div class="sectionContent">
			<img src="AboutPageAssets/teasers/robustvggt.png" alt="DiffTrack Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle">Emergent Outlier View Rejection in Visual Geometry Grounded Transformer</h2>

      		<h3 class="sectionContentSubTitle"><a>Jisang Han<sup>*</sup></a>, <a>Sunghwan Hong<sup>*</sup></a>, <a>Jaewoo Jung</a>, <a>Wooseok Jang</a>, <b><a>Honggyu An</a></b>, <a>Qianqian Wang</a>, <a>Seungryong Kim<sup>&dagger;</sup></a>, <a>Chen Feng<sup>&dagger;</sup></a></h3>
		  	<h3 class="sectionContentSubTitle"><em> arXiv, 2025 </em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://cvlab-kaist.github.io/RobustVGGT/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>TL;DR</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">We reveal that Visual Geometry Grounded Transformers (VGGT) has a built-in ability to detect outliers, which we leverage to perform outlier-view rejection without any fine-tuning. </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2512.04012" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/cvlab-kaist/RobustVGGT" target="_blank"><b>Code</b></a></div>
		</aside>

	  <br><br>



		<!-- VIRAL -->
		<div class="sectionContent">
			<img src="AboutPageAssets/teasers/viral.png" alt="DiffTrack Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle">VIRAL: Visual Representation Alignment for Multimodal Large Language Models</h2>

      		<h3 class="sectionContentSubTitle"><a>Heeji Yoon<sup>*</sup></a>, <a>Jaewoo Jung<sup>*</sup></a>, <a>Junwan Kim<sup>*</sup></a>, <a>Hyungyu Choi</a>, <a>Heeseong Shin</a>, <a>Sangbeom Lim</a>, <b><a>Honggyu An</a></b>, <a>Chaehyun Kim</a>, <a>Jisang Han</a>, <a>Donghyun Kim</a>, <a>Chanho Eom</a>, <a>Sunghwan Hong</a>, <a>Seungryong Kim</a></h3>
		  	<h3 class="sectionContentSubTitle"><em> arXiv, 2025 </em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://cvlab-kaist.github.io/VIRAL/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>TL;DR</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">VIRAL directly aligns MLLM's visual tokens with pretrained vision model features, preserving fine-grained visual details and boosting vision-centric reasoning beyond text-only supervision. </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2509.07979" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/cvlab-kaist/VIRAL" target="_blank"><b>Code</b></a></div>
		</aside>

	  <br><br>



	  	  <!-- Lumiere  -->
			<div class="sectionContent">
				<img align="center" src="AboutPageAssets/teasers/ddust3r.png" style="height: 100%; width: 65%; object-fit: contain"  alt="">
				</div>
			<section class="section2Content">
					<h2 class="sectionContentTitle"> D<sup>2</sup>USt3R: Enhancing 3D Reconstruction with 4D Pointmaps for Dynamic Scenes </h2>
					<h3 class="sectionContentSubTitle">Jisang Han*, <strong>Honggyu An*</strong>, Jaewoo Jung*, Takuya Narihira, Junyoung Seo, Kazumi Fukuda, Chaehyun Kim, Sunghwan Hong, Yuki Mitsufuji, Seungryong Kim
					</h3>
					<h3 class="sectionContentSubTitle"><em>arXiv, 2025<b style="color:darksalmon"></b>
					</em></h3>
			</section>
			<aside class="externalResourcesNav">
	
				<div class="dropdown"><span></span><a href="https://cvlab-kaist.github.io/DDUSt3R/" target="_blank"><b>Project Page</b></a></div>
				<div class="dropdown"> <span>Abstract</span>
				<div class="dropdown-content">
					<p style="text-align:left;">We address the task of 3D reconstruction in dynamic scenes, where object motions degrade the quality of 3D pointmap regression methods, such as DUSt3R, originally designed for static 3D scene reconstruction. Although these methods provide an elegant and powerful solution in static settings, they struggle in the presence of dynamic motions that disrupt alignment based solely on camera poses. To overcome this, we propose D2USt3R that regresses 4D pointmaps that simultaneiously capture both static and dynamic 3D scene geometry in a feed-forward manner. By explicitly incorporating both spatial and temporal aspects, our approach successfully encapsulates spatio-temporal dense correspondence to the proposed 4D poitnmaps, enhancing performance of downstream tasks. Extensive experimental evaluations demonstrate that our proposed approach consistently achieves superior reconstruction performance across various datasets featuring complex motions.</p></div>
				</div>
				<div class="dropdown"><span></span><a href="https://arxiv.org/abs/2504.06264" target="_blank"><b>Paper</b></a></div>
				<div class="dropdown"><span></span><a href="https://cvlab-kaist.github.io/DDUSt3R/" target="_blank"><b></b>Code</b></a></div>
			</aside>
	
			<br><br>
			<br><br>


		<div class="sectionContent">
			<img align="center" src="AboutPageAssets/teasers/zeroco.png" style="height: 100%; width: 100%; object-fit: contain"  alt="">
			</div>
		<section class="section2Content">
				<h2 class="sectionContentTitle"> Cross-View Completion Models are Zero-shot Correspondence Estimators </h2>
				<h3 class="sectionContentSubTitle"><b>Honggyu An*</b>, Jinhyeon Kim*, Seonghoon Park, Jaewoo Jung, Jisang Han, Sunghwan Hong, Seungryong Kim
				</h3>
				<h3 class="sectionContentSubTitle"><em>CVPR, <strong>Highlight</strong> (3.0% Acceptance Rate), 2025<b style="color:darksalmon"></b>
				</em></h3>
		</section>
		<aside class="externalResourcesNav">

			<div class="dropdown"><span></span><a href="https://cvlab-kaist.github.io/ZeroCo/" target="_blank"><b>Project Page</b></a></div>
			<div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
				<p style="text-align:left;">In this work, we explore new perspectives on cross-view completion learning by drawing an analogy to self-supervised correspondence learning. Through our analysis, we demonstrate that the cross-attention map within cross-view completion models captures correspondence information more effectively than other correlations derived from encoder or decoder features. We verify the effectiveness of the cross-attention map by evaluating on both zero-shot matching and learning-based geometric matching and multi-frame depth estimation.</p></div>
			</div>
			<div class="dropdown"><span></span><a href="https://arxiv.org/abs/2412.09072" target="_blank"><b>Paper</b></a></div>
			<div class="dropdown"><span></span><a href="https://github.com/cvlab-kaist/ZeroCo" target="_blank"><b>Code</b></a></div>
		</aside>

		<br><br>
		<br><br>


		<div class="sectionContent">
			<img align="center" src="AboutPageAssets/teasers/locotrack.png" style="height: 100%; width: 100%; object-fit: contain"  alt="">
			</div>
		<section class="section2Content">
				<h2 class="sectionContentTitle"> Local All-Pair Correspondence for Point Tracking </h2>
				<h3 class="sectionContentSubTitle">Seokju Cho, Jiahui Huang, Jisu Nam, <b>Honggyu An</b>, Seungryong Kim<sup>&dagger;</sup>, Joon-Young Lee<sup>&dagger;</sup>
				</h3>
				<h3 class="sectionContentSubTitle"><em>ECCV, 2024<b style="color:darksalmon"></b>
				</em></h3>
		</section>
		<aside class="externalResourcesNav">

			<div class="dropdown"><span></span><a href="https://cvlab-kaist.github.io/locotrack/" target="_blank"><b>Project Page</b></a></div>
			<div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
				<p style="text-align:left;">We introduce LocoTrack, a highly accurate and efficient model designed for the task of tracking any point (TAP) across video sequences. Previous approaches in this task often rely on local 2D correlation maps to establish correspondences from a point in the query image to a local region in the target image, which often struggle with homogeneous regions or repetitive features, leading to matching ambiguities. LocoTrack overcomes this challenge with a novel approach that utilizes all-pair correspondences across regions, i.e., local 4D correlation, to establish precise correspondences, with bidirectional correspondence and matching smoothness significantly enhancing robustness against ambiguities. We also incorporate a lightweight correlation encoder to enhance computational efficiency, and a compact Transformer architecture to integrate long-term temporal information. LocoTrack achieves unmatched accuracy on all TAP-Vid benchmarks and operates at a speed almost 6 times faster than the current state-of-the-art.</p></div>
			</div>
			<div class="dropdown"><span></span><a href="https://arxiv.org/abs/2407.15420" target="_blank"><b>Paper</b></a></div>
			<div class="dropdown"><span></span><a href="https://github.com/cvlab-kaist/locotrack" target="_blank"><b>Code</b></a></div>
		</aside>

		<br><br>
		<br><br>

		<div class="sectionContent">
			<img align="center" src="AboutPageAssets/teasers/rain_gs.png" style="height: 100%; width: 100%; object-fit: contain"  alt="">
	  	</div>
		<section class="section2Content">
      		<h2 class="sectionContentTitle"> RAIN-GS: Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting</h2>
      		<h3 class="sectionContentSubTitle">Jaewoo Jung*, Jisang Han*, <b>Honggyu An*</b>, Jiwon Kang*, Seonghoon Park*, Seungryong Kim
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>arXiv, 2024<b style="color:darksalmon"></b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/RAIN-GS/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>Abstract</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">3D Gaussian splatting (3DGS) has recently demonstrated impressive capabilities in real-time novel view synthesis and 3D reconstruction. However, 3DGS heavily depends on the accurate initialization derived from Structure-from-Motion (SfM) methods. When trained with randomly initialized point clouds, 3DGS often fails to maintain its ability to produce high-quality images, undergoing large performance drops of 4-5 dB in PSNR in general. Through extensive analysis of SfM initialization in the frequency domain and analysis of a 1D regression task with multiple 1D Gaussians, we propose a novel optimization strategy dubbed RAIN-GS (Relaxing Accurate INitialization Constraint for 3D Gaussian Splatting) that successfully trains 3D Gaussians from randomly initialized point clouds. We show the effectiveness of our strategy through quantitative and qualitative comparisons on standard datasets, largely improving the performance in all settings.</p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="http://arxiv.org/abs/2403.09413" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/RAIN-GS" target="_blank"><b>Code</b></a></div>
		</aside>

	  <br><br>
	  <br><br>


	  	<div class="sectionContent">
	 		<img align="center" src="AboutPageAssets/teasers/maskingdepth.png" style="height: 100%; width: 100%; object-fit: contain"  alt="">

	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle">MaskingDepth: Masked Consistency Regularization for Semi-supervised Monocular Depth Estimation</h2>
      		<h3 class="sectionContentSubTitle">Jongbeom Baek*,Gyeongnyeon Kim*, Seonghoon Park*,<b>Honggyu An</b>,Matteo Poggi, Seungryong Kim
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024<b style="color:darksalmon"></b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/MaskingDepth/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>Abstract</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">
				We propose MaskingDepth, a novel semi-supervised learning framework for monocular depth estimation to mitigate the reliance on large ground-truth depth quantities. MaskingDepth is designed to enforce consistency between the strongly-augmented unlabeled data and the pseudo-labels derived from weakly-augmented unlabeled data, which enables learning depth without supervision. In this framework, a novel data augmentation is proposed to take the advantage of a na ̈ıve masking strategy as an augmentation, while avoiding its scale ambiguity problem between depths from weakly- and strongly-augmented branches and risk of missing small-scale instances. To only retain high-confident depth predictions from the weakly augmented branch as pseudo-labels, we also present an uncertainty estimation technique, which is used to define robust consistency regularization. Experiments on KITTI and NYU-Depth-v2 datasets demonstrate the effectiveness of each component, its robustness to the use of fewer depth- annotated images, and superior performance compared to other state-of-the-art semi-supervised methods for monocular depth estimation. Furthermore, we show our method can be easily extended to domain adaptation task.
			  </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2212.10806" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/MaskingDepth" target="_blank"><b>Code</b></a></div>
		</aside>

	  <br><br>
	  <br><br>
	  <!-- SMM  -->
	  	<div class="sectionContent">
	 		<img align="center" src="AboutPageAssets/teasers/DaRF.png" style="height: 100%; width: 100%; object-fit: contain"  alt="">
	  	</div>
		<section class="section2Content">
      		<h2 class="sectionContentTitle"> DäRF: Boosting Few-shot Neural Radiance Field with Joint Monocular Depth Adaptation</h2>
      		<h3 class="sectionContentSubTitle">Jiuhn Song*, Seonghoon Park*, <b>Honggyu An*</b>, Seokju Cho, Min-Seop Kwak, Sungjin Cho, Seungryong Kim
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>Neural Information Processing Systems (Neurips), 2023<b style="color:darksalmon"></b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/DaRF/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>Abstract</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">Neural radiance fields (NeRF) shows powerful performance in novel view synthesis and 3D geometry reconstruction, but it suffers from critical performance degradation when the number of known viewpoints is drastically reduced. Existing works attempt to overcome this problem by employing external priors, but their success is limited to certain types of scenes or datasets. Employing monocular depth estimation (MDE) networks, pretrained on large-scale RGB-D datasets, with powerful generalization capability would be a key to solving this problem: however, using MDE in conjunction with NeRF comes with a new set of challenges due to various ambiguity problems exhibited by monocular depths. In this light, we propose a novel framework, dubbed DäRF, that achieves robust NeRF reconstruction with a handful of real-world images by combining the strengths of NeRF and monocular depth estimation through online complementary training. Our framework imposes the MDE network's powerful geometry prior to NeRF representation at both seen and unseen viewpoints to enhance its robustness and coherence. In addition, we overcome the ambiguity problems of monocular depths through patch-wise scale-shift fitting and geometry distillation, which adapts the MDE network to produce depths aligned accurately with NeRF geometry. Experiments show our framework achieves state-of-the-art results both quantitatively and qualitatively, demonstrating consistent and reliable performance in both indoor and outdoor real-world datasets.</p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2305.19201" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/DaRF" target="_blank"><b>Code</b></a></div>
		</aside>

	  <br><br>
	  <br><br>

	  <!-- SMM  -->

	
<!-- Replicate the above Div block to add more title and company details --> 
	</section>
<!--	<ul style="color: slategrey">-->
<!--  	<li>Splicing ViT Features: TAU Graphics Seminar 2022</li>-->
<!--  	<li>Text2LIVE: HUJI Vision Seminar 2022</li>-->
<!--	<li>Text2LIVE: Israel Computer Vision day 2022</li>-->
<!--	<li>Unveiling new priors for re-rendering images and videos: TAU Deep Learning Seminar 2023</li>-->
<!--	<li>Text2LIVE: Viz.ai 2023</li>-->
<!--</ul>-->
</section>
<footer>
	<br><br>
  <p class="footerDisclaimer">Template of <a href="https://lioryariv.github.io/">Lior Yariv</a><span></span></p>
  <p class="footerNote"></p>
</footer>
</body>
</html>
